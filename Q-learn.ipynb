{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the maze layout\n",
    "maze = np.array([\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 1, 0, 1, 0, 1, 1],\n",
    "    [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 1, 1, 1, 0, 1],\n",
    "    [1, 0, 1, 'G', 0, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0, 1, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 'E', 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "# Define starting position and goals\n",
    "start = (1, 1)  # Coordinates of 'S'\n",
    "goal_sub = (5, 3)  # Coordinates of 'G'\n",
    "goal_end = (8, 7)  # Coordinates of 'E'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     action \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m))  \u001b[39m# Explore\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(q_table[state[\u001b[39m0\u001b[39;49m], state[\u001b[39m1\u001b[39;49m]])  \u001b[39m# Exploit\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Take action and observe new state and reward\u001b[39;00m\n\u001b[1;32m     42\u001b[0m next_state \u001b[39m=\u001b[39m (state[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m actions[action][\u001b[39m0\u001b[39m], state[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m actions[action][\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((10, 10, 4))  # 4 actions: up, down, left, right\n",
    "\n",
    "# Action mapping\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n",
    "\n",
    "def is_valid_move(maze, position):\n",
    "    x, y = position\n",
    "    return 0 <= x < maze.shape[0] and 0 <= y < maze.shape[1] and maze[x, y] != 1\n",
    "\n",
    "def get_reward(position):\n",
    "    if position == goal_sub:\n",
    "        return 10  # Reward for reaching the sub-goal\n",
    "    elif position == goal_end:\n",
    "        return 100  # Reward for reaching the end goal\n",
    "    return -1  # Penalty for each step\n",
    "\n",
    "# Training the Q-learning agent\n",
    "for episode in range(num_episodes):\n",
    "    state = start\n",
    "    while True:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(range(4))  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[0], state[1]])  # Exploit\n",
    "\n",
    "        # Take action and observe new state and reward\n",
    "        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
    "        \n",
    "        if is_valid_move(maze, next_state):\n",
    "            reward = get_reward(next_state)\n",
    "            # Update Q-value\n",
    "            q_table[state[0], state[1], action] += alpha * (reward + gamma * np.max(q_table[next_state[0], next_state[1]]) - q_table[state[0], state[1], action])\n",
    "            state = next_state\n",
    "        else:\n",
    "            reward = -1  # Penalty for invalid move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_to_goal_sub = []\n",
    "steps_to_goal_end = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = start\n",
    "    steps = 0\n",
    "    reached_sub_goal = False\n",
    "    \n",
    "    while True:\n",
    "        action = np.argmax(q_table[state[0], state[1]])\n",
    "        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
    "        \n",
    "        if is_valid_move(maze, next_state):\n",
    "            steps += 1\n",
    "            if not reached_sub_goal and next_state == goal_sub:\n",
    "                reached_sub_goal = True\n",
    "            if reached_sub_goal and next_state == goal_end:\n",
    "                steps_to_goal_end.append(steps)\n",
    "                break\n",
    "            state = next_state\n",
    "        else:\n",
    "            steps += 1  # Count penalties for invalid moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reached time limit.\n",
      "Episode 2 reached time limit.\n",
      "Episode 3 reached time limit.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # Import time library\n",
    "\n",
    "# Define the maze layout\n",
    "maze = np.array([\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 1, 0, 1, 0, 1, 1],\n",
    "    [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 1, 1, 1, 0, 1],\n",
    "    [1, 0, 1, 'G', 0, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0, 1, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 'E', 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "# Define starting position and goals\n",
    "start = (1, 1)  # Coordinates of 'S'\n",
    "goal_sub = (5, 3)  # Coordinates of 'G'\n",
    "goal_end = (8, 7)  # Coordinates of 'E'\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000\n",
    "time_limit = 5  # Time limit in seconds for each episode\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((10, 10, 4))  # 4 actions: up, down, left, right\n",
    "\n",
    "# Action mapping\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n",
    "\n",
    "def is_valid_move(maze, position):\n",
    "    x, y = position\n",
    "    return 0 <= x < maze.shape[0] and 0 <= y < maze.shape[1] and maze[x, y] != 1\n",
    "\n",
    "def get_reward(position):\n",
    "    if position == goal_sub:\n",
    "        return 10  # Reward for reaching the sub-goal\n",
    "    elif position == goal_end:\n",
    "        return 100  # Reward for reaching the end goal\n",
    "    return -1  # Penalty for each step\n",
    "\n",
    "# Training the Q-learning agent\n",
    "for episode in range(num_episodes):\n",
    "    state = start\n",
    "    steps = 0  # Track steps taken in this episode\n",
    "    reached_sub_goal = False\n",
    "    start_time = time.time()  # Start timing the episode\n",
    "\n",
    "    while True:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(range(4))  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[0], state[1]])  # Exploit\n",
    "\n",
    "        # Take action and observe new state and reward\n",
    "        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
    "        \n",
    "        if is_valid_move(maze, next_state):\n",
    "            reward = get_reward(next_state)\n",
    "            # Update Q-value\n",
    "            q_table[state[0], state[1], action] += alpha * (reward + gamma * np.max(q_table[next_state[0], next_state[1]]) - q_table[state[0], state[1], action])\n",
    "            state = next_state\n",
    "            \n",
    "            steps += 1  # Count steps\n",
    "            if next_state == goal_sub:\n",
    "                reached_sub_goal = True\n",
    "            if next_state == goal_end:\n",
    "                print(f\"Episode {episode + 1} completed in {steps} steps.\")\n",
    "                break\n",
    "        else:\n",
    "            reward = -1  # Penalty for invalid move\n",
    "            steps += 1  # Count penalty for invalid moves\n",
    "\n",
    "        # Check for time limit\n",
    "        if time.time() - start_time > time_limit:\n",
    "            print(f\"Episode {episode + 1} reached time limit.\")\n",
    "            break\n",
    "\n",
    "# Function to visualize the maze and agent's path\n",
    "def visualize_maze(maze, path=None):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(maze, cmap='gray_r')\n",
    "    if path is not None:\n",
    "        path_x, path_y = zip(*path)\n",
    "        plt.plot(path_y, path_x, color='yellow', linewidth=3)  # Path taken by the agent\n",
    "    plt.scatter(start[1], start[0], c='blue', label='Start (S)', s=100)\n",
    "    plt.scatter(goal_sub[1], goal_sub[0], c='green', label='Sub-goal (G)', s=100)\n",
    "    plt.scatter(goal_end[1], goal_end[0], c='red', label='End goal (E)', s=100)\n",
    "    plt.xticks(np.arange(10))\n",
    "    plt.yticks(np.arange(10))\n",
    "    plt.gca().invert_yaxis()  # Invert y axis to match maze layout\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Maze Navigation with Q-learning')\n",
    "    plt.show()\n",
    "\n",
    "# Find the path taken to the end goal\n",
    "def find_path(q_table, start):\n",
    "    state = start\n",
    "    path = [state]\n",
    "    while state != goal_end:\n",
    "        action = np.argmax(q_table[state[0], state[1]])\n",
    "        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
    "        if is_valid_move(maze, next_state):\n",
    "            path.append(next_state)\n",
    "            state = next_state\n",
    "        else:\n",
    "            break  # Break if no valid moves (though this shouldn't happen)\n",
    "    return path\n",
    "\n",
    "# Find the path taken to the end goal\n",
    "final_path = find_path(q_table, start)\n",
    "\n",
    "# Visualize the maze with the agent's final path\n",
    "visualize_maze(maze, path=final_path)\n",
    "\n",
    "# Visualizing Q-values\n",
    "def visualize_q_values(q_table):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if maze[i, j] == 1:  # Wall positions\n",
    "                continue\n",
    "            plt.subplot(10, 10, i * 10 + j + 1)\n",
    "            plt.bar(range(4), q_table[i, j])\n",
    "            plt.ylim(0, np.max(q_table))\n",
    "            plt.title(f'({i},{j})')\n",
    "            plt.xticks(range(4), ['Up', 'Down', 'Left', 'Right'])\n",
    "            plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Q-values for Each State (Action Value Estimates)', y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the Q-values\n",
    "visualize_q_values(q_table)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
